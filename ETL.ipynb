{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified ETL Pipeline\n",
    "\n",
    "This notebook reproduces the steps from the Python ETL scripts in a single,\n",
    "well-documented workflow. It generates the same output tables as the original\n",
    "scripts for ONET, AEI and CAGED data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import basedosdados as bd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# OK\n",
    "def strip_accents(text: str) -> str:\n",
    "    \"\"\"Return ``text`` without accent marks.\"\"\"\n",
    "\n",
    "    # 1) Decompose characters into base + combining marks (NFKD)\n",
    "    #    e.g. \"á\" → \"á\"  (two code-points)\n",
    "    decomposed = unicodedata.normalize(\"NFKD\", text)\n",
    "    # 2) Keep only the base characters (category != Mn = \"Mark, Non-spacing\")\n",
    "    return \"\".join(ch for ch in decomposed if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "\n",
    "def merge_onet_soc_data() -> pd.DataFrame:\n",
    "    \"\"\"Merge O*NET task statements with SOC major group titles.\n",
    "\n",
    "    The CSV paths are fixed inside the function and point to the files in\n",
    "    ``data/input/aei_data``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with task statements and their corresponding SOC major group\n",
    "        titles.\n",
    "    \"\"\"\n",
    "    onet_df = pd.read_csv('data/input/aei_data/onet_task_statements.csv')\n",
    "    onet_df['soc_major_group'] = onet_df['O*NET-SOC Code'].str[:2]\n",
    "\n",
    "    soc_df = pd.read_csv('data/input/aei_data/SOC_Structure.csv')\n",
    "    soc_df = soc_df.dropna(subset=['Major Group'])\n",
    "    soc_df['soc_major_group'] = soc_df['Major Group'].str[:2]\n",
    "\n",
    "    merged = onet_df.merge(\n",
    "        soc_df[['soc_major_group', 'SOC or O*NET-SOC 2019 Title']],\n",
    "        on='soc_major_group',\n",
    "        how='left'\n",
    "    )\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_pct = pd.read_csv('data/input/aei_data/task_pct_v2.csv')\n",
    "automation_vs_augmentation_by_task = pd.read_csv(\n",
    "    'data/input/aei_data/automation_vs_augmentation_by_task.csv'\n",
    ")\n",
    "onet_tasks = pd.read_csv('data/input/aei_data/onet_task_statements.csv')\n",
    "soc_structure_aei = pd.read_csv('data/input/aei_data/SOC_Structure.csv')\n",
    "\n",
    "soc_structure_full = pd.read_csv('data/input/SOC_Structure.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CAGED\n",
    "load_dotenv()\n",
    "LIMIT = False\n",
    "\n",
    "\n",
    "# To re-run the query from the original data source, uncomment the lines below\n",
    "# billing_id = os.getenv('BILLING_ID')\n",
    "# with open('data/config/caged_bd_national.SQL') as f:\n",
    "#     query = f.read()\n",
    "#     if LIMIT:\n",
    "#         query += f\" LIMIT {LIMIT}\"\n",
    "# df = bd.read_sql(query=query, billing_project_id=billing_id)\n",
    "# df.to_parquet('data/input/caged_national_UNTREATED.parquet')\n",
    "\n",
    "caged_raw = pd.read_parquet('data/input/caged_national_UNTREATED.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ONET ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soc_structure_full['minor_group'] = soc_structure_full.apply(\n",
    "    lambda row: row.dropna().iloc[0][:4], axis=1\n",
    ")\n",
    "\n",
    "# Groups by minor group, aggregating \"SOC or O*NET-SOC 2019 Title\" by concatenating the strings with '; '\n",
    "minor_groups = (\n",
    "    soc_structure_full.groupby('minor_group')\n",
    "    .agg({'SOC or O*NET-SOC 2019 Title': lambda x: '; '.join(x)})\n",
    "    .rename({'SOC or O*NET-SOC 2019 Title': 'title'}, axis='columns')\n",
    ")\n",
    "minor_groups.to_csv('data/output/onet_minor_groups.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AEI ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f3fa1",
   "metadata": {},
   "source": [
    "Generate AEI occupation level metrics.\n",
    "\n",
    "This script merges Anthropic Economic Index (AEI) task data with the\n",
    "Standard Occupational Classification (SOC) structure in order to compute\n",
    "automation and augmentation ratios by SOC minor group. The resulting\n",
    "tables are written to ``data/output`` for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onet_with_soc = merge_onet_soc_data()\n",
    "onet_with_soc['task_normalized'] = onet_with_soc['Task'].str.lower().str.strip()\n",
    "\n",
    "onet_with_soc['n_occurrences'] = (\n",
    "    onet_with_soc.groupby('task_normalized')['Title'].transform('nunique')\n",
    ")\n",
    "\n",
    "grouped_with_occ = task_pct.merge(\n",
    "    onet_with_soc, left_on='task_name', right_on='task_normalized', how='left'\n",
    ")\n",
    "\n",
    "grouped_with_occ['pct_occ_scaled'] = 100 * (\n",
    "    grouped_with_occ['pct'] / grouped_with_occ['n_occurrences']\n",
    ") / (\n",
    "    grouped_with_occ['pct'] / grouped_with_occ['n_occurrences']\n",
    ").sum()\n",
    "\n",
    "automation_vs_augmentation_with_occ = grouped_with_occ.merge(\n",
    "    automation_vs_augmentation_by_task, on='task_name', how='left'\n",
    ")\n",
    "assert len(automation_vs_augmentation_with_occ) == len(grouped_with_occ)\n",
    "\n",
    "onet_tasks_tmp = onet_tasks[['O*NET-SOC Code', 'Task']].copy()\n",
    "onet_tasks_tmp['soc_minor_group'] = onet_tasks_tmp['O*NET-SOC Code'].str[:4]\n",
    "onet_tasks_tmp['task_name'] = onet_tasks_tmp['Task'].str.lower().str.strip()\n",
    "onet_tasks_tmp = onet_tasks_tmp[['soc_minor_group', 'task_name']].drop_duplicates()\n",
    "\n",
    "df = task_pct.merge(automation_vs_augmentation_by_task, on='task_name', how='left')\n",
    "df.fillna(0, inplace=True)\n",
    "df['aug'] = (df['learning'] + df['validation']) * df['pct']\n",
    "df['aut'] = (df['feedback_loop'] + df['directive'] + df['task_iteration']) * df['pct']\n",
    "df.drop(columns=['learning', 'validation', 'feedback_loop', 'directive', 'task_iteration'], inplace=True)\n",
    "\n",
    "df = df.merge(onet_tasks_tmp, on='task_name', how='left')\n",
    "occ_aut_aug_lvl = df.groupby('soc_minor_group')[['pct', 'aug', 'aut']].sum().reset_index()\n",
    "occ_aut_aug_lvl['aug_aut_ratio'] = occ_aut_aug_lvl['aug'] / occ_aut_aug_lvl['aut']\n",
    "occ_aut_aug_lvl.to_parquet('data/output/occ_aut_aug_lvl.parquet')\n",
    "\n",
    "\n",
    "# Classifying jobs\n",
    "## 1. Highest usage jobs\n",
    "top_10_pct = occ_aut_aug_lvl.sort_values('pct', ascending=False).head(10)\n",
    "top_10_pct['class'] = 'Top 10 pct'\n",
    "\n",
    "## 2. Lowest usage jobs\n",
    "bottom_10_pct = occ_aut_aug_lvl.sort_values('pct', ascending=True).head(10)\n",
    "bottom_10_pct['class'] = 'Bottom 10 pct'\n",
    "\n",
    "## 3. jobs with highest aut\n",
    "top_10_aut = occ_aut_aug_lvl.sort_values('aut', ascending=False).head(10)\n",
    "top_10_aut['class'] = 'Top 10 aut'\n",
    "\n",
    "## 4. jobs with highest aug\n",
    "top_10_aug = occ_aut_aug_lvl.sort_values('aug', ascending=False).head(10)\n",
    "top_10_aug['class'] = 'Top 10 aug'\n",
    "\n",
    "classified = pd.concat([top_10_pct, bottom_10_pct, top_10_aut, top_10_aug])\n",
    "classified.to_parquet('data/output/occ_aut_aug_lvl_classified.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CAGED ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = caged_raw.copy()\n",
    "df = df[~df['cbo_2002_descricao_subgrupo_principal'].isna()]\n",
    "\n",
    "cols_to_keep = [\n",
    "    'ano',\n",
    "    'mes',\n",
    "    'cbo_2002_descricao_subgrupo_principal',\n",
    "    'cbo_2002_descricao_grande_grupo'\n",
    "]\n",
    "\n",
    "df = (\n",
    "    df.groupby(cols_to_keep, dropna=False)['saldo_movimentacao']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    ")\n",
    "\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        'saldo_movimentacao': 'net_jobs',\n",
    "        'ano': 'year',\n",
    "        'mes': 'month',\n",
    "        'cbo_2002_descricao_subgrupo_principal': 'cbo_subgroup',\n",
    "        'cbo_2002_descricao_grande_grupo': 'cbo_group'\n",
    "    }\n",
    ")\n",
    "\n",
    "df['cbo_subgroup'] = df['cbo_subgroup'].apply(strip_accents)\n",
    "df['cbo_group'] = df['cbo_group'].apply(strip_accents)\n",
    "\n",
    "df.to_parquet('data/input/caged_national.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
