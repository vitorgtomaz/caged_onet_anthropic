{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified ETL Pipeline\n",
    "\n",
    "This notebook reproduces the steps from the Python ETL scripts in a single,\n",
    "well-documented workflow. It generates the same output tables as the original\n",
    "scripts for ONET, AEI and CAGED data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import basedosdados as bd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# OK\n",
    "def strip_accents(text: str) -> str:\n",
    "    \"\"\"Return ``text`` without accent marks.\"\"\"\n",
    "\n",
    "    # 1) Decompose characters into base + combining marks (NFKD)\n",
    "    #    e.g. \"\u00e1\" \u2192 \"a\u0301\"  (two code-points)\n",
    "    decomposed = unicodedata.normalize(\"NFKD\", text)\n",
    "    # 2) Keep only the base characters (category != Mn = \"Mark, Non-spacing\")\n",
    "    return \"\".join(ch for ch in decomposed if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "\n",
    "def load_onet_tasks_with_soc() -> pd.DataFrame:\n",
    "    \"\"\"Merge O*NET task statements with SOC major group titles.\n",
    "\n",
    "    The CSV paths are fixed inside the function and point to the files in\n",
    "    ``data/input/aei_data``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with task statements and their corresponding SOC major group\n",
    "        titles.\n",
    "    \"\"\"\n",
    "    onet_df = pd.read_csv('data/input/aei_data/onet_task_statements.csv')\n",
    "    onet_df['soc_major_group'] = onet_df['O*NET-SOC Code'].str[:2]\n",
    "\n",
    "    soc_df = pd.read_csv('data/input/aei_data/SOC_Structure.csv')\n",
    "    soc_df = soc_df.dropna(subset=['Major Group'])\n",
    "    soc_df['soc_major_group'] = soc_df['Major Group'].str[:2]\n",
    "\n",
    "    merged = onet_df.merge(\n",
    "        soc_df[['soc_major_group', 'SOC or O*NET-SOC 2019 Title']],\n",
    "        on='soc_major_group',\n",
    "        how='left'\n",
    "    )\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading\n",
    "\n",
    "This section loads all datasets required for the ETL process.\n",
    "The first code cell reads AEI task usage percentages, task classifications\n",
    "and the SOC structure files. The second cell loads the CAGED dataset from\n",
    "a cached parquet file (or queries it from BigQuery if the code is uncommented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of workers using each task\n",
    "task_usage_pct = pd.read_csv('data/input/aei_data/task_usage_pct_v2.csv')\n",
    "# Automation vs augmentation scores for each task\n",
    "auto_aug_by_task = pd.read_csv(\n",
    "    'data/input/aei_data/auto_aug_by_task.csv'\n",
    ")\n",
    "# Raw O*NET task statements\n",
    "onet_tasks = pd.read_csv('data/input/aei_data/onet_task_statements.csv')\n",
    "# SOC titles linked to each task\n",
    "soc_structure_tasks = pd.read_csv('data/input/aei_data/SOC_Structure.csv')\n",
    "\n",
    "# Full SOC hierarchy used for ONET lookup\n",
    "soc_structure_full = pd.read_csv('data/input/SOC_Structure.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CAGED\n",
    "# Load environment variables for BigQuery access\n",
    "load_dotenv()\n",
    "# Set a row limit when re-running the SQL query\n",
    "LIMIT = False\n",
    "\n",
    "\n",
    "# To re-run the query from the original data source, uncomment the lines below\n",
    "# billing_id = os.getenv('BILLING_ID')\n",
    "# with open('data/config/caged_bd_national.SQL') as f:\n",
    "#     query = f.read()\n",
    "#     if LIMIT:\n",
    "#         query += f\" LIMIT {LIMIT}\"\n",
    "# df = bd.read_sql(query=query, billing_project_id=billing_id)\n",
    "# df.to_parquet('data/input/caged_national_UNTREATED.parquet')\n",
    "\n",
    "# Cached CAGED query result\n",
    "caged_raw = pd.read_parquet('data/input/caged_national_UNTREATED.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ONET ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the minor group prefix from each row\n",
    "soc_structure_full['minor_group'] = soc_structure_full.apply(\n",
    "    lambda row: row.dropna().iloc[0][:4], axis=1\n",
    ")\n",
    "\n",
    "# Groups by minor group, aggregating \"SOC or O*NET-SOC 2019 Title\" by concatenating the strings with '; '\n",
    "# Aggregate SOC titles by minor group\n",
    "minor_groups = (\n",
    "    soc_structure_full.groupby('minor_group')\n",
    "    .agg({'SOC or O*NET-SOC 2019 Title': lambda x: '; '.join(x)})\n",
    "    .rename({'SOC or O*NET-SOC 2019 Title': 'title'}, axis='columns')\n",
    ")\n",
    "minor_groups.to_csv('data/output/onet_minor_groups.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AEI ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f3fa1",
   "metadata": {},
   "source": [
    "Generate AEI occupation level metrics.\n",
    "\n",
    "This script merges Anthropic Economic Index (AEI) task data with the\n",
    "Standard Occupational Classification (SOC) structure in order to compute\n",
    "automation and augmentation ratios by SOC minor group. The resulting\n",
    "tables are written to ``data/output`` for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Merge AEI tasks with SOC data\n",
    "This step combines task statements with SOC titles and computes usage percentages per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined task statements with SOC titles\n",
    "tasks_with_soc = load_onet_tasks_with_soc()\n",
    "# Normalized task names for joining\n",
    "tasks_with_soc['task_normalized'] = tasks_with_soc['Task'].str.lower().str.strip()\n",
    "\n",
    "tasks_with_soc['num_occupations'] = (\n",
    "    tasks_with_soc.groupby('task_normalized')['Title'].transform('nunique')\n",
    ")\n",
    "\n",
    "# Merge usage percentages with task metadata\n",
    "task_usage = task_usage_pct.merge(\n",
    "    tasks_with_soc, left_on='task_name', right_on='task_normalized', how='left'\n",
    ")\n",
    "\n",
    "# Scale usage by how many occupations perform the task\n",
    "task_usage['pct_occ_scaled'] = 100 * (\n",
    "    task_usage['pct'] / task_usage['num_occupations']\n",
    ") / (\n",
    "    task_usage['pct'] / task_usage['num_occupations']\n",
    ").sum()\n",
    "\n",
    "# Add automation vs augmentation labels\n",
    "auto_aug_usage = task_usage.merge(\n",
    "    auto_aug_by_task, on='task_name', how='left'\n",
    ")\n",
    "assert len(auto_aug_usage) == len(task_usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Aggregate metrics by occupation\n",
    "Using the merged task data we compute weighted automation and augmentation scores for each SOC minor group.\n",
    "Scores are calculated by summing the task-level contributions after weighting them by task usage percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each task to its SOC minor group\n",
    "minor_task_lookup = onet_tasks[['O*NET-SOC Code', 'Task']].copy()\n",
    "# Minor group code from the SOC code\n",
    "minor_task_lookup['soc_minor_group'] = minor_task_lookup['O*NET-SOC Code'].str[:4]\n",
    "# Normalized task name\n",
    "minor_task_lookup['task_name'] = minor_task_lookup['Task'].str.lower().str.strip()\n",
    "# Map each task to its SOC minor group\n",
    "# Keep unique task to minor group mappings\n",
    "minor_task_lookup = minor_task_lookup[['soc_minor_group', 'task_name']].drop_duplicates()\n",
    "\n",
    "# Combine usage and automation/augmentation info\n",
    "task_metrics = task_usage_pct.merge(auto_aug_by_task, on='task_name', how='left')\n",
    "# Replace missing scores with zero\n",
    "task_metrics.fillna(0, inplace=True)\n",
    "# Weighted augmentation score\n",
    "task_metrics['aug'] = (task_metrics['learning'] + task_metrics['validation']) * task_metrics['pct']\n",
    "# Weighted automation score\n",
    "task_metrics['aut'] = (task_metrics['feedback_loop'] + task_metrics['directive'] + task_metrics['task_iteration']) * task_metrics['pct']\n",
    "# Remove intermediate columns\n",
    "task_metrics.drop(columns=['learning', 'validation', 'feedback_loop', 'directive', 'task_iteration'], inplace=True)\n",
    "\n",
    "# Combine usage and automation/augmentation info\n",
    "# Attach minor group info to each task\n",
    "task_metrics = task_metrics.merge(minor_task_lookup, on='task_name', how='left')\n",
    "# Sum metrics for each minor group\n",
    "occupation_metrics = task_metrics.groupby('soc_minor_group')[['pct', 'aug', 'aut']].sum().reset_index()\n",
    "# Ratio of augmentation to automation\n",
    "occupation_metrics['aug_aut_ratio'] = occupation_metrics['aug'] / occupation_metrics['aut']\n",
    "# Save aggregated metrics\n",
    "occupation_metrics.to_parquet('data/output/occ_aut_aug_lvl.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Classify occupations\n",
    "Based on the aggregated metrics we identify occupations with extreme values.\n",
    "The following lists capture the occupations with the highest and lowest overall task usage,\n",
    "as well as those most associated with automation or augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classifying jobs\n",
    "## 1. Highest usage jobs\n",
    "# Occupations where AEI tasks are most prevalent\n",
    "highest_pct_occupations = occupation_metrics.sort_values('pct', ascending=False).head(10)\n",
    "highest_pct_occupations['class'] = 'Top 10 pct'\n",
    "\n",
    "## 2. Lowest usage jobs\n",
    "# Occupations where AEI tasks are least common\n",
    "lowest_pct_occupations = occupation_metrics.sort_values('pct', ascending=True).head(10)\n",
    "lowest_pct_occupations['class'] = 'Bottom 10 pct'\n",
    "\n",
    "## 3. jobs with highest aut\n",
    "# Occupations with the greatest automation usage\n",
    "highest_aut_occupations = occupation_metrics.sort_values('aut', ascending=False).head(10)\n",
    "highest_aut_occupations['class'] = 'Top 10 aut'\n",
    "\n",
    "## 4. jobs with highest aug\n",
    "# Occupations with the greatest augmentation usage\n",
    "highest_aug_occupations = occupation_metrics.sort_values('aug', ascending=False).head(10)\n",
    "highest_aug_occupations['class'] = 'Top 10 aug'\n",
    "\n",
    "# Combine all rankings into a single table\n",
    "classified_occupations = pd.concat([highest_pct_occupations, lowest_pct_occupations, highest_aut_occupations, highest_aug_occupations])\n",
    "classified_occupations.to_parquet('data/output/occ_aut_aug_lvl_classified.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CAGED ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean CAGED job data\n",
    "caged = caged_raw.copy()\n",
    "# Remove rows missing occupation information\n",
    "caged = caged[~caged['cbo_2002_descricao_subgrupo_principal'].isna()]\n",
    "\n",
    "# Columns used for aggregation\n",
    "cols_to_keep = [\n",
    "    'ano',\n",
    "    'mes',\n",
    "    'cbo_2002_descricao_subgrupo_principal',\n",
    "    'cbo_2002_descricao_grande_grupo'\n",
    "]\n",
    "\n",
    "# Aggregate monthly job flows\n",
    "caged = (\n",
    "    caged.groupby(cols_to_keep, dropna=False)['saldo_movimentacao']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "caged = caged.rename(\n",
    "    columns={\n",
    "        'saldo_movimentacao': 'net_jobs',\n",
    "        'ano': 'year',\n",
    "        'mes': 'month',\n",
    "        'cbo_2002_descricao_subgrupo_principal': 'cbo_subgroup',\n",
    "        'cbo_2002_descricao_grande_grupo': 'cbo_group'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Normalize occupation names\n",
    "caged['cbo_subgroup'] = caged['cbo_subgroup'].apply(strip_accents)\n",
    "caged['cbo_group'] = caged['cbo_group'].apply(strip_accents)\n",
    "\n",
    "# Persist the cleaned CAGED dataset\n",
    "caged.to_parquet('data/input/caged_national.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
