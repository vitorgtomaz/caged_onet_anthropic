{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified ETL Pipeline\n",
    "\n",
    "This notebook reproduces the steps from the Python ETL scripts in a single,\n",
    "well-documented workflow. It generates the same output tables as the original\n",
    "scripts for ONET, AEI and CAGED data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b267977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text: str) -> str:\n",
    "    \"\"\"Return ``text`` without accent marks.\"\"\"\n",
    "    decomposed = unicodedata.normalize(\"NFKD\", text)\n",
    "    return \"\".join(ch for ch in decomposed if unicodedata.category(ch) != \"Mn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b7b96d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "CW_ONET_CBO = pd.read_csv(\n",
    "    \"data/config/cbo_onet_crosswalks.csv\", encoding=\"utf-8\", engine=\"python\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aei(file_name):\n",
    "    \"\"\"Load data from Antrhopic Economic Data's Huggingace Repository\"\"\"\n",
    "    return pd.read_csv(\n",
    "                       hf_hub_download(\n",
    "                            repo_id=\"Anthropic/EconomicIndex\",\n",
    "                            filename=f\"release_2025_03_27/{file_name}\",\n",
    "                            repo_type=\"dataset\"\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "# Loads the two Anthropic Economic Index databases.\n",
    "# Check https://arxiv.org/pdf/2503.04761#page=16&zoom=100,110,189 for more details on the methodology\n",
    "tasks_aei       = load_aei(\"task_pct_v2.csv\") # Database with two columns: task_name (as per onet) and pct (percentage of conversations attributed to the task)\n",
    "aut_aug_by_task = load_aei(\"automation_vs_augmentation_by_task.csv\") # Database that attributes each conversations to a mode of use. \n",
    "\n",
    "# Loads the two official ONET databses\n",
    "onet_df         = load_aei(\"onet_task_statements.csv\") # Contains task metadata, linking it to occupations\n",
    "soc_structure   = load_aei(\"SOC_Structure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fad9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CAGED\n",
    "\n",
    "#########################################################################################\n",
    "# To re-run the query from the original data source, uncomment the lines below ##########\n",
    "#########################################################################################\n",
    "\n",
    "# load_dotenv()\n",
    "# LIMIT = False\n",
    "# import basedosdados as bd\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# billing_id = os.getenv('BILLING_ID')\n",
    "# with open('data/config/caged_bd_national.SQL') as f:\n",
    "#     query = f.read()\n",
    "#     if LIMIT:\n",
    "#         query += f\" LIMIT {LIMIT}\"\n",
    "# df = bd.read_sql(query=query, billing_project_id=billing_id)\n",
    "# df.to_parquet('data/input/caged_national_UNTREATED.parquet')\n",
    "\n",
    "caged_raw = pd.read_parquet('data/input/caged_national_UNTREATED.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ONET ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies soc_structure, keeping only the rows referring to minor groups, renames the columns\n",
    "# and adds a columnn major_group based on the first two characters of minor_group\n",
    "s = soc_structure.copy()\\\n",
    "    .rename(\n",
    "        {'SOC or O*NET-SOC 2019 Title': 'major_title',\n",
    "         'Minor Group': 'minor_group'}, axis='columns')\\\n",
    "    .dropna(subset=['minor_group'])\\\n",
    "    .assign(minor_group=lambda x: x['minor_group'].str[:4])\\\n",
    "    [['minor_group', 'major_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AEI ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f3fa1",
   "metadata": {},
   "source": [
    "Generate AEI occupation level metrics.\n",
    "\n",
    "This script merges Anthropic Economic Index (AEI) task data with the\n",
    "Standard Occupational Classification (SOC) structure in order to compute\n",
    "automation and augmentation ratios by SOC minor group. The resulting\n",
    "tables are written to ``data/output`` for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Merge AEI tasks with SOC data\n",
    "This step combines task statements with SOC titles and computes usage percentages per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges the ONET task statements with the SOC structure, assigning major and minor groups and titles\n",
    "tasks_df = (\n",
    "    onet_df\n",
    "        .copy()\n",
    "        # Extracts major and minor group codes (first two and four characters respectively)\n",
    "        .assign(minor_group=lambda x: x[\"O*NET-SOC Code\"].str[:4])\n",
    "        .assign(major_group=lambda x: x['minor_group'].str[:2])\n",
    "        \n",
    "        # Normalizes task names to match format in Anthropic Economic Index data\n",
    "        .assign(task = lambda x: x['Task'].str.lower().str.strip())\n",
    "        \n",
    "        # Merges with other datasets:\n",
    "        ## SOC structure, to get the Major and Minor Group titles\n",
    "        .merge(s, on=\"minor_group\", how=\"left\")\n",
    "        \n",
    "        ## AEI data on the percentage of conversations assigned to each task\n",
    "        .merge(tasks_aei, left_on='task', right_on='task_name', how='inner')\n",
    "        \n",
    "        ## AEI data on classification of use by automation vs. augmentation\n",
    "        .merge(aut_aug_by_task, on='task_name', how='left')\n",
    "        \n",
    "        # Distributes the percentages evenly per occupation and calculates the percentage of total\n",
    "        # conversations per task and occupation. In future iterations, distribution can be improved and adapted\n",
    "        .assign(n_occ = lambda x: x.groupby('task')['Title'].transform('nunique'))\n",
    "        .assign(pct_total = lambda x: 100 * (x['pct'] / x['n_occ']) / (x['pct'] / x['n_occ']).sum())\n",
    "        \n",
    "        # Sums the percentages attributed to subclassifications of automation and augmentation\n",
    "        # As per the methodology in the AEI paper, augmentation = learning + validation\n",
    "        # and automation = feedback_loop + directive + task_iteration\n",
    "        .assign(aug = lambda x: (x['learning'] + x['validation']) * x['pct_total'])\n",
    "        .assign(aut = lambda x: (x['feedback_loop'] + x['directive'] + x['task_iteration']) * x['pct'])\n",
    "        \n",
    "        # Keeps only the columns of interest\n",
    "        [['major_group', 'minor_group', 'major_title', 'Title', 'task', 'n_occ', 'pct_total', 'aut', 'aug']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8494ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that merges and calculations were done correctly\n",
    "assert len(tasks_df[tasks_df['major_title'].isna()]) == 0\n",
    "assert len(tasks_df[tasks_df['n_occ'] == 0]) == 0\n",
    "assert len(tasks_df[tasks_df['n_occ'].isna()]) == 0\n",
    "assert tasks_df['pct_total'].sum() == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb15d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tasks: 3364\n",
      "Number of minor groups: 91\n",
      "Number of major groups: 22\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tasks:', len(tasks_df['task'].unique()))\n",
    "print('Number of minor groups:', len(tasks_df['minor_group'].unique()))\n",
    "print('Number of major groups:', len(tasks_df['major_group'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Aggregate metrics by occupation\n",
    "Combine task usage with automation labels and summarize them by SOC minor group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occupation_metrics: aggregate task metrics to the minor group level\n",
    "occupation_df = (\n",
    "    tasks_df\n",
    "        .groupby('minor_group')[['pct_total', 'aug', 'aut']]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .assign(aug_aut_ratio = lambda x: x['aug'] / x['aut'])\n",
    "        .assign(top_10        = lambda x: (x['pct_total'].rank(method='first', ascending=False) <= 10).astype(int))\n",
    "        .assign(bottom_10     = lambda x: (x['pct_total'].rank(method='first', ascending=True)  <= 10).astype(int))\n",
    "        .assign(top_10_aut    = lambda x: (      x['aut'].rank(method='first', ascending=False) <= 10).astype(int))\n",
    "        .assign(top_10_aug    = lambda x: (      x['aug'].rank(method='first', ascending=False) <= 10).astype(int))\n",
    "        \n",
    ")\n",
    "\n",
    "occupation_df.to_parquet('data/output/occ_aut_aug_lvl.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates final_occupations.csv, containing the list of 40 records comprising each 'top 10' category\n",
    "\n",
    "final_occupations = pd.concat(\n",
    "        [\n",
    "            occupation_df[occupation_df['top_10'] == 1    ].assign(cat = 'top_10'),\n",
    "            occupation_df[occupation_df['bottom_10'] == 1 ].assign(cat = 'bottom_10'),\n",
    "            occupation_df[occupation_df['top_10_aug'] == 1].assign(cat = 'top_10_aug'),\n",
    "            occupation_df[occupation_df['top_10_aut'] == 1].assign(cat = 'top_10_aut')\n",
    "        ],\n",
    "        axis=0\n",
    "    )\n",
    "final_occupations.to_csv('data/output/final_occupations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CAGED ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "caged = (\n",
    "    caged_raw.copy()\n",
    "    .dropna(subset=['cbo_2002_descricao_subgrupo_principal'])\n",
    "    .groupby(['ano',\n",
    "              'mes',\n",
    "              'cbo_2002_descricao_subgrupo_principal',\n",
    "              'cbo_2002_descricao_grande_grupo'\n",
    "            ], \n",
    "            dropna=False\n",
    "    )['saldo_movimentacao'].sum().reset_index()\n",
    "    .apply(lambda col: col.str.strip() if col.dtype == 'object' else col)\n",
    "    .rename(\n",
    "        columns={\n",
    "            'saldo_movimentacao': 'net_jobs',\n",
    "            'ano': 'year',\n",
    "            'mes': 'month',\n",
    "            'cbo_2002_descricao_subgrupo_principal': 'cbo_subgroup',\n",
    "            'cbo_2002_descricao_grande_grupo': 'cbo_group'\n",
    "        }\n",
    "    )\n",
    "    .assign(cbo_subgroup = lambda x: x['cbo_subgroup'].apply(strip_accents))\n",
    "    .assign(cbo_group = lambda x: x['cbo_group'].apply(strip_accents))\n",
    ")\n",
    "\n",
    "\n",
    "caged.to_parquet('data/output/caged_national.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d8a4c",
   "metadata": {},
   "source": [
    "# 6. Combine CAGED and AEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f067c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CAGED job data with AEI occupation classifications.\n",
    "\n",
    "CW_ONET_CBO[\"cbo_subgroup\"] = CW_ONET_CBO[\"cbo_subgroup\"].apply(strip_accents)\n",
    "\n",
    "df = (\n",
    "    caged\n",
    "        .merge(CW_ONET_CBO, how=\"left\", on=\"cbo_subgroup\")\n",
    "        .merge(final_occupations, how=\"left\", left_on=\"onet_code\", right_on=\"minor_group\")\n",
    "        #.dropna(subset=['cat'])\n",
    "        .assign(date = lambda x: pd.to_datetime(x[\"year\"].astype(str) + x[\"month\"].astype(str), format=\"%Y%m\"))\n",
    "        .groupby([\"date\", \"cat\"])[\"net_jobs\"].sum().reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "time_series = df.groupby([\"date\", \"cat\"])[\"net_jobs\"].sum().reset_index()\n",
    "time_series.to_csv(\"data/output/time_series.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28219831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
